ðŸš¨ SirenSense

SirenSense is an AI-powered mobile application designed to improve urban accessibility and safety by detecting critical environmental sounds (like sirens, alarms, and horns) in real time. The app empowers users with instant alerts, vibration patterns, and community-driven mapping of sound events.

ðŸ“± Tech Stack

Frontend Application:

Built with Flutter for a smooth, cross-platform mobile experience.

AI/ML Engine:

TensorFlow Lite (TFLite) integrated with YAMNet, a pre-trained audio model capable of classifying 500+ real-world sounds.

Database Layer:

Firebase Firestore for secure, real-time data storage and instant updates across devices.

Backend Services:

Firebase Functions for lightweight processing, scalable cloud tasks, and emergency alarm triggers.

Mapping & Visualization:

Google Maps API / Mapbox for displaying live community sound events and emergency alerts with intuitive markers.

Alerts & Notifications:

On-device vibration patterns and notifications.

Firebase Cloud Messaging (FCM) for push alerts, including notifying nearby users during emergencies.

âœ¨ Unique Features

Custom Vibration Patterns

Three distinct vibration modes allow users to instantly recognize critical sounds such as horns, sirens, or alarms without confusion.

On-Device AI Processing

Sound detection occurs locally using TensorFlow Lite.

Ensures low latency, offline functionality, and complete privacy (no raw audio leaves the device).

Community-Driven Sound Map

Anonymous contributions build a live, city-wide sound accessibility map.

Pioneers safer and more inclusive urban spaces.

Emergency Alarm Button

Allows users to notify nearby people for help.

Once accepted, others are informed that assistance is on the way.

Scalable and Future-Ready

Starts at the campus level with seamless scalability to smart cities.

Potential for nationwide adoption as a standard for accessibility.
